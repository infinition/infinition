Breaking GPU Dependency: Recursive VLA for Edge RoboticsDate: 12 Oct 2025Author: Fabien PollyThe Problem with Modern VLAVisual-Language-Action (VLA) models like RT-2 represent the future of embodied AI. They allow robots to understand natural language commands and translate them into physical actions.However, current SOTA models are massive (billions of parameters) and require heavy GPU clusters for inference. This creates a significant latency bottleneck for autonomous systems operating on the edge (drones, rovers, home robots) where internet connection isn't guaranteed.My Approach: Tiny Recursive NetworksInstead of processing the entire high-resolution visual field at once through a Vision Transformer (ViT), my research focuses on Recursive Attention:Low-Res Scan: The model takes a quick, low-res look at the scene.ROI Selection: A tiny policy network decides where to look closer.High-Res Patch: Only the relevant patch is processed at high resolution.Key Findings & ResultsBy implementing this architecture in Rust with bindings for Python, I achieved significant improvements on limited hardware:300% Inference Speed Increase on a Raspberry Pi 5 (compared to quantized LLaVA).92% Accuracy on grasping tasks in simulated environments (PyBullet).Zero Cloud Dependency: The robot remains fully autonomous.Next StepsI am currently working on porting the "World Model" component to run on the Hailo-8 AI accelerator to further reduce power consumption.Link to Repository